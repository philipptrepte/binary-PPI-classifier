---
title: "Introduction to binaryPPIclassifier"
author: Philipp Trepte
date: "`r format(Sys.Date(), '%Y-%m-%d')`"
package: binaryPPIclassifier
output: 
  BiocStyle::html_document:
    toc: true
    toc_float: true
vignette: >
  %\VignetteIndexEntry{Introduction to binaryPPIclassifier}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>", 
  fig.width = 6, 
  fig.height = 4
)
```

```{r requiredPackages, echo = FALSE, warning=FALSE, results="hide"}
suppressPackageStartupMessages({
  library(tidyverse)
library(DT)
library(ggpubr)
library(viridis)
library(binaryPPIclassifier)
library(varhandle)
library(ggnewscale)
library(cowplot)
})

```

\pagebreak

# Using machine learning (ML) algorithms to classify quantitative PPI results

This vignette gives an introduction to the `binaryPPIclassifier` package. A **support vector machine learning** or **random forest** algorithm in combination with multi-adaptive sampling is trained on a data set containing positive and random/negative reference protein-protein interaction (PPI) results. The generated models are then applied to a set of PPIs with unknown classification and their likelihood to be positive or negative is predicted.

# Installation

You can install the development version of binaryPPIclassifier from [GitHub](https://github.com/) with:

``` r
# install.packages("devtools")
devtools::install_github("philipptrepte/binary-PPI-classifier")

library(binaryPPIclassifier)
```

# Requirements

The input data should have been pre-processed or tidied if necessary.

## Example dataset: LuTHy results from PRS-v2 and RRS-v2

We used the LuTHy assay ([Trepte et al. Mol Sys Biol 2018](https://www.embopress.org/doi/full/10.15252/msb.20178071)) to detect protein-protein interactions between 60 PRS-v2 and 78 RRS-v2 ([Choi et al. Nat Com 2019](https://www.nature.com/articles/s41467-019-11809-2)), which were each tested in 8 tagging configutation with the LuTHy-BRET and LuTHy-LuC assay versions ([Trepte & Secker et al. bioRxiv 2023](https://www.biorxiv.org/content/10.1101/2023.06.14.544560v1)).

The dataset contains *in-cell* LuTHy-BRET and *cell-free* LuTHy-LuC measurements. For the LuTHy-BRET we measure the Acceptor protein expression *mean_mCit*, which directly influences the BRET ratio *mean_cBRET*. For the LuTHy-LuC, we measure the raw luminescence value after precipitation *mean_LumiOUT* of the Donor protein which is normalized to its expression resulting in the *mean_cLuC*.

For more details on the LuTHy assay and its measurement parameters, please read:

1.  Trepte, P. *et al.* LuTHy: a double-readout bioluminescence-based two-hybrid technology for quantitative mapping of protein--protein interactions in mammalian cells. *Mol Syst Biol* **14**, e8071 (2018). [[Link](https://www.embopress.org/doi/full/10.15252/msb.20178071)]

2.  Trepte, P. & Secker, C. *et al.* AI-guided pipeline for protein-protein interaction drug discovery identifies a SARS-CoV-2 inhibitor. *bioRxiv* 2023.06.14.544560 (2023) <doi:10.1101/2023.06.14.544560>. [[Link](https://www.biorxiv.org/content/10.1101/2023.06.14.544560v1)]

## Loading of the data

```{r loadData, echo=TRUE, message=FALSE, warning=FALSE}
data("luthy_reference_sets")
```

```{r inputTable, echo=FALSE, message=FALSE, warning=FALSE, paged.print=TRUE}
knitr::kable(luthy_reference_sets %>% select(-plate, -PPI) %>% filter(interaction == "BAD + BCL2L1"),
             caption = 'Input table showing results for the interaction BAD + BCL2L1 from `luthy_reference_sets`.')
```

------------------------------------------------------------------------

| Column name      | Description                                                                                                                                                                                   |
|------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Donor            | unique id to identify the donor construct                                                                                                                                                     |
| Donor_tag        | indicates if the donor protein is tagged N-terminally ("**N**") or C-terminally ("**C**")                                                                                                     |
| Donor_protein    | a protein identifier, e.g. BCL2L1                                                                                                                                                             |
| Acceptor         | unique id to identify the acceptor construct                                                                                                                                                  |
| Acceptor_tag     | indicates if the acceptor protein is tagged N-terminally ("**N**") or C-terminally ("**C**")                                                                                                  |
| Acceptor_protein | a protein identifier, e.g. BAD                                                                                                                                                                |
| complex          | should contain information if it is a reference interaction (e.g. "PRS"/"RRS") or if it is part of any other known complex (e.g. "LAMTOR")                                                    |
| interaction      | should indicate the tested interaction independent on the tagging configuration, e.g. "BAD + BCL2L1"                                                                                          |
| sample           | should indicate the tested interaction dependent on the tagging configuration, e.g. "BCL2L1 + BCL2L1"                                                                                         |
| orientation      | indicates the exact tagging orientation of the first indicated donor protein (e.g. N1) and the second indicated acceptor protein (e.g. N2): N1-N2 for BAD + BCL2L1 and N2-N1 for BCL2L1 + BAD |
| data             | indicates the type of data measured for an indicated interaction, e.g. "mean_cBRET" or "mean_cLuC"                                                                                            |
| score            | The corresponding data value                                                                                                                                                                  |

: (#tab:reqCol) Required table columns. Additional metadata columns are optional, e.g. UniProt ids

## Example interaction: BAD + BCL2L1

| Tagging configuration (`orientation` column) | Example (NL = NanoLuc; mCit = mCitrine) | interaction  | sample       |
|----------------------------------------------|-----------------------------------------|--------------|--------------|
| C1-C2                                        | BAD-NL + BCL2L1-mCit                    | BAD + BCL2L1 | BAD + BCL2L1 |
| C1-N2                                        | BAD-NL + mCit-BCL2L1                    | BAD + BCL2L1 | BAD + BCL2L1 |
| N1-C2                                        | NL-BAD + BCL2L1-mCit                    | BAD + BCL2L1 | BAD + BCL2L1 |
| N1-N2                                        | NL-BAD + mCit-BCL2L1                    | BAD + BCL2L1 | BAD + BCL2L1 |
| C2-C1                                        | BCL2L1-NL + BAD-mCit                    | BAD + BCL2L1 | BCL2L1 + BAD |
| C2-N1                                        | BCL2L1-NL + mCit-BAD                    | BAD + BCL2L1 | BCL2L1 + BAD |
| N2-C1                                        | NL-BCL2L1 + BAD-mCit                    | BAD + BCL2L1 | BCL2L1 + BAD |
| N2-N1                                        | NL-BCL2L1 + mCit-BAD                    | BAD + BCL2L1 | BCL2L1 + BAD |

: (#tab:TableTaggingOrientation) Tagging configurations and formatting of interaction and sample column.

### LuTHy-BRET

```{r luthyBretExample, echo=FALSE, message=FALSE, warning=FALSE, fig.cap="LuTHy-BRET results for one positive (BAD + BCL2L1) and one random (ARSA + DBN1) PPI"}
luthy_reference_sets %>% 
  filter(interaction == "BAD + BCL2L1" & data == "mean_cBRET" |
           interaction == "ARSA + DBN1" & data == "mean_cBRET" |
           interaction == "BAD + BCL2L1" & data == "mean_mCit" |
           interaction == "ARSA + DBN1" & data == "mean_mCit") %>%
  dplyr::mutate(interaction = paste0(complex, ": ", interaction)) %>%
  ggplot(aes(x = orientation, y = score, fill = score)) +
  geom_bar(stat = "identity", fill = "#6CA6C1") +
  theme_pubr() +
  facet_grid(data ~ interaction, scales = "free_y") +
  ggplot2::theme(text = element_text(family = "Avenir"),
          plot.title = element_text(size = 12),
          plot.subtitle = element_text(size = 8),
          axis.title = element_text(family = "Avenir Medium"),
          axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)) +
  labs(y = "LuTHy-BRET measurements", x = "tagging configuration")
```

# Usage

The ML algorithms for the LuTHy-BRET and LuTHy-LuC are trained on the *mean_cBRET* and *mean_mCit* or *mean_cLuC* and *mean_LumiOUT* features, respectively. Other assays that provide only one quantitative measurement, like the mN2H, can be trained only on this one feature. Training on more than two features is also possible but has not been tested.

| Assay      | Training features (`data` column) |
|------------|-----------------------------------|
| LuTHy-BRET | mean_cBRET, mean_mCit             |
| LuTHy-LuC  | mean_cLuC, mean_LumiOUT           |

## ppi.prediction()

The function `ppi.prediction` is used to predict the classification probability of a `PPIdf` with unknown classification labels by training an `svm` or `randomforest` machine learning algorithm on a set of reference interactions that contain classification labels. The parameters to be specified for the function can be found in the Documentation `?ppi.prediction.`

In the following example, we use the `luthy_reference_set` example data to compile distinct training sets by sampling 50 times (`ensembleSize` = 50) from the entire `referenceSet` dataset. Weighted sampling is performed (`sampling` = "weighted") using the mean_cBRET data (`weightBy` = "mean_cBRET") from all interactions (`cutoff` = "all"). The data will not be further scaled (`method.scaling` = "none") and we specify that in the complex column the negative/random interactions are indicated by "RRS" (`negative.reference` = "RRS"). We further specify that as training features for the LuTHy-BRET the mean_cBRET and mean_mCit (`assay` = c("mean_cBRET", "mean_mCit") will be used to train the SVM algorithms (`model.type` = "svm"). For the SVM algorithm a linear kernel will be used (`kernelType` = "linear") with a cost of constraints violation of 100 (`C` = 100). During training, the class labels of the reference set are reclassified in 5 iterations (`iter` = 5)

``` r
example_ppi_prediction <- ppi.prediction(PPIdf = luthy_reference_sets, 
               referenceSet = luthy_reference_sets,
               ensembleSize = 50,
               sampling = "weighted",
               weightBy = "mean_cBRET",
               cutoff = "all",
               method.scaling = "none",
               negative.reference = "RRS",
               assay = ("mean_cBRET", "mean_mCit"),
               model.type = "svm",
               kernelType = "linear",
               C = 100,
               iter = 5)
```

The resulting list contains the following objects:

| List object        | Description                                                                                                                                                     |
|--------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------|
| predTrainDf        | Data frame of the training set containing the predicted classifier probabilities in the column `predTrainMat`                                                   |
| predDf             | Data frame of the PPIdf test set containing the predicted classifier probabilities in the column `predMat`                                                      |
| predTrain.model.e  | List for each `ensembleSize` as defined in `ppi.prediction()` containing the trained ML models used to predict the classification of the reference training set |
| training.sets      | List for each `ensembleSize` as defined in `ppi.prediction()` containing the training sets used to train each ML model                                          |
| negative.reference | as specified in `ppi.prediction()`                                                                                                                              |
| model.type         | as specified in `ppi.prediction()`                                                                                                                              |
| model.e            | List for each `ensembleSize` as defined in `ppi.prediction()` containing the trained ML models used to predict the classification of the PPIdf test set         |
| testMat            | Matrix of the PPIdf test set containing the features used for training                                                                                          |
| trainMat           | Matrix of the reference training set containing the features used for training                                                                                  |
| label              | Classifier labels of the reference training set interactions                                                                                                    |
| cutoff             | as specified in `ppi.prediction()`                                                                                                                              |
| inclusion          | as specified in `ppi.prediction()`                                                                                                                              |
| ensembleSize       | as specified in `ppi.prediction()`                                                                                                                              |
| sampling           | as specified in `ppi.prediction()`                                                                                                                              |
| kernelType         | as specified in `ppi.prediction()`                                                                                                                              |
| iter               | as specified in `ppi.prediction()`                                                                                                                              |
| C                  | as specified in `ppi.prediction()`                                                                                                                              |
| gamma              | as specified in `ppi.prediction()`                                                                                                                              |
| coef0              | as specified in `ppi.prediction()`                                                                                                                              |
| degree             | as specified in `ppi.prediction()`                                                                                                                              |
| top                | as specified in `ppi.prediction()`                                                                                                                              |
| seed               | as specified in `ppi.prediction()`                                                                                                                              |
| system.time        | System time when the function was run                                                                                                                           |

### Plotting the results from the ML predictions

The package provides helper functions, to plot the results from the ML predictions (\@ref(fig:MLresultPlot)). `probGrid.plot` will generate a probability grid, `probDis.plot` will show the probability distribution against the first feature and a receiver operating characteristic curve can be plotted using the `roc.plot` function. Finally, the recovery rates at 50%, 75% and 95% can be plotted using the function `recovery.plot`.

```{r MLresultPlot, echo=TRUE, message=FALSE, warning=FALSE, fig.height=7.5, fig.width=10, fig.cap="Training results as (A) probability grid, (B) probability distribtuoin, (C) ROC curve plotting the training features and the probability, (D) recovery rates at 50%, 75% and 95% probability"}
data("example_ppi_prediction")
cowplot::plot_grid(
    probGrid.plot(example_ppi_prediction, ylim = c(-0.1, 0.8)),
    probDis.plot(example_ppi_prediction),
    roc.plot(example_ppi_prediction),
    recovery.plot(example_ppi_prediction),
  ncol = 2, align = "hv", axis = "tlrb", labels = "AUTO"
)

```

### Confusion Matrix

To evaluate the performance of the classification model, we can calculate the confusion matrix using the function `confusion.matrix` which is based on the `caret::confusionMatrix` function.

```{r exampleConfMat, echo=TRUE, message=FALSE, warning=FALSE}
example_confusion_matrix <- confusion.matrix(example_ppi_prediction)
```

A tabulated confusion matrix can be accessed by `example_confusion_matrix$confusionMatrixDf`

```{r exampleConfMatDf, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(example_confusion_matrix$confusionMatrixDf[,1:2], caption = "Tabulated confusion matrix.")
```

Detailed information and statistics can be assessed by `example_confusion_matrix$confusionMatrixList` for at 50%, 75% and 95% probability.

```{r exampleConfMatLis, echo=FALSE, message=FALSE, warning=FALSE}
example_confusion_matrix$confusionMatrixList$`95%`
```

### Example interaction: BAD + BCL2L1

```{r predExamplePPI, echo=FALSE, message=FALSE, warning=FALSE, fig.height=4, fig.width=5, fig.cap="(A) Predicted probabilities to be true-positive interactions and (B) mean cBRET values for the PRS-v2 interaction BAD + BCL2L1 and the RRS-v2 interaction ARSA + DBN1"}

cowplot::plot_grid(
  example_ppi_prediction$predDf %>%
  filter(interaction == "BAD + BCL2L1" |
           interaction == "ARSA + DBN1" |
           interaction == "BAD + BCL2L1" |
           interaction == "ARSA + DBN1") %>%
  ggplot(aes(x = orientation, y = interaction, fill = predMat)) +
  geom_tile() +
  scale_fill_gradientn(colours = c("#FBF49C", "#ff6b6b", "#f7fff7", "#48C2C5"), 
                       limits = c(0,1), na.value = "#FBF49C") +
  theme_pubr() +
  ggplot2::theme(text = element_text(family = "Avenir"),
          plot.title = element_text(size = 12),
          plot.subtitle = element_text(size = 8),
          axis.title = element_text(family = "Avenir Medium"),
          axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1),
          legend.position = "right") +
  labs(fill = "probability"),
  
  example_ppi_prediction$predDf %>%
  filter(interaction == "BAD + BCL2L1" |
           interaction == "ARSA + DBN1" |
           interaction == "BAD + BCL2L1" |
           interaction == "ARSA + DBN1") %>%
  ggplot(aes(x = orientation, y = interaction, fill = mean_cBRET)) +
  geom_tile() +
  scale_fill_gradientn(colours = c("#FBF49C", "#ff6b6b", "#f7fff7", "#48C2C5"), 
                       na.value = "#FBF49C") +
  theme_pubr() +
  ggplot2::theme(text = element_text(family = "Avenir"),
          plot.title = element_text(size = 12),
          plot.subtitle = element_text(size = 8),
          axis.title = element_text(family = "Avenir Medium"),
          axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1),
          legend.position = "right") +
  labs(fill = "mean cBRET"), 
  ncol = 1, labels = "AUTO"
)

```

## learning.curve()

To evaluate the model performance, we can plot learning curves using the function `learning.curve()`. Performance metrics (accuracy, hinge loss, binary cross-entropy loss) are plotted against the amount of training data. Thereby, it can be determined if the model is training effectively or if more data would improve performance. Learning curves also help to evaluate over- and underfitting of the models.

The function `learning.curve` only requires a `ppi.prediction` object as input. Additionally, the relative training sizees can be specified for example as `train_size = base::seq(0.1, 1, by = 0.1)` and the number of models used from `ppi.prediction$model.e` can be specified as `models = 10` to use the first 10 models or `models = "all"` to use all saved models.

``` r
example_learning_curve <- learning.curve(ppi_prediction_result = example_ppi_prediction)
```

```{r exampleLearningCurve, message=FALSE, warning=FALSE, fig.cap="(A) Accuracy, (B) Hinge Loss, (C) Binary Cross-Entropy Loss", fig.width=7, fig.height=3}
example_learning_curve$learning_plot
```

# Appendix

## Session info

```{r sessionInfo, echo=FALSE}
sessionInfo()
```
